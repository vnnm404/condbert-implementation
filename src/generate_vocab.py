from transformers import BertTokenizer
import numpy as np
import pickle
import os
from collections import defaultdict
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import make_pipeline
import pickle
from tqdm.auto import tqdm


'''
Input files (Both generated by dataset_create.py)

1) ../data/train_toxic.txt
2) ../data/train_normal.txt

Output files

1) ../vocab/negative_words.txt
2) ../vocab/positive_words.txt
3) ../vocab/word2weight.pkl
4) ../vocab/token_toxicities.txt

This script needs to be only run once

'''

# Creating vocab directory
vocab_dir = "../vocab/"
if not os.path.exists(vocab_dir):
    os.makedirs(vocab_dir)

# Locations of input files
train_toxic_path = "../data/train_toxic.txt"
train_normal_path ="../data/train_normal.txt"

# Reading the toxic and the normal txt file
with open(train_toxic_path, 'r') as tox_corpus, open(train_normal_path, 'r') as norm_corpus:
    corpus_tox = [line.strip() for line in tox_corpus.readlines()]
    corpus_norm = [line.strip() for line in norm_corpus.readlines()]


'''
Below is the code to generate the first 2 output files

1) ../vocab/negative_words.txt
2) ../vocab/positive_words.txt

'''


neg_word_path = os.path.join(vocab_dir, "negative_words.txt")
pos_word_path = os.path.join(vocab_dir, "positive_words.txt")




vectorizer = CountVectorizer(ngram_range=(1,1))
tox_count_matrix = vectorizer.fit_transform(corpus_tox)
tox_vocab = vectorizer.vocabulary_
tox_counts = np.sum(tox_count_matrix, axis=0)

norm_count_matrix = vectorizer.fit_transform(corpus_norm)
norm_vocab = vectorizer.vocabulary_
norm_counts = np.sum(norm_count_matrix, axis=0)


# helper functions to help get select negative and positive words

def _get_count(feature, vocab, counts):
    if feature not in vocab:
        return 0.0
    else:
        return counts[0, vocab[feature]]

def tox_score(feature, lmbda=0.5):
    
    tox_count = _get_count(feature, tox_vocab, tox_counts)
    norm_count = _get_count(feature, norm_vocab, norm_counts)

    return (tox_count + lmbda) / (norm_count + lmbda) 

def get_combined_vocabulary(tox_corpus, norm_corpus):
    
    vectorizer = CountVectorizer(ngram_range=(1,1))
    combined_corpus = tox_corpus + norm_corpus  
    count_matrix = vectorizer.fit_transform(combined_corpus)
    combined_vocab = vectorizer.vocabulary_

    return combined_vocab

visited = set()
threshold = 4 # same threshold the authors use in paper
combined_vocab = get_combined_vocabulary(corpus_tox,corpus_norm)

with open(neg_word_path, 'w') as neg_out, open(pos_word_path, 'w') as pos_out:
    for token in combined_vocab.keys():
        if token not in visited:
            visited.add(token)
            neg_score = tox_score(token)
            if neg_score > threshold:
                neg_out.writelines(f'{token}\n')
            elif 1/neg_score > threshold:
                pos_out.writelines(f'{token}\n')


'''

Below is the code to generate the third output file

3) ../vocab/word2weight.pkl

'''

word2weight_path = os.path.join(vocab_dir,"word2weight.pkl")

# Setting up the pipeline for Logistic Regrssion clasifier 
pipe = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))

# training the model
X_train = corpus_tox + corpus_norm
y_train = [1] * len(corpus_tox) + [0] * len(corpus_norm)
pipe.fit(X_train, y_train)

# getting the weights of each word
weights = pipe[1].coef_[0]

word2weight = {w: weights[idx] for w, idx in pipe[0].vocabulary_.items()}

with open(word2weight_path, 'wb') as f:
    pickle.dump(word2weight, f)


'''

Below is the code to generate the third output file

4) ../vocab/token_toxicities.txt

'''

token_toxicities_path = os.path.join(vocab_dir,"token_toxicities.txt")

# Loading Bert Tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)

toxic_counter = defaultdict(lambda: 1)
nontoxic_counter = defaultdict(lambda: 1)

# Calculating the number of toxic and normal occurences for each token
for text in tqdm(corpus_tox):
    for token in tokenizer.encode(text):
        toxic_counter[token] += 1
for text in tqdm(corpus_norm):
    for token in tokenizer.encode(text):
        nontoxic_counter[token] += 1

token_toxicities = [toxic_counter[i] / (nontoxic_counter[i] + toxic_counter[i]) for i in range(len(tokenizer.vocab))]
with open(token_toxicities_path, 'w') as f:
    for t in token_toxicities:
        f.write(str(t))
        f.write('\n')

print("Done!\n")








